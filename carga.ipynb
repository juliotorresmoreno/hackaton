{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applications.csv       Candidates.csv\t    Psychometrics.csv  Vacants.csv\r\n",
      "ApplicationStages.csv  LinkRetoTecnico.txt  Stages.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "import statistics\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"hackaton2\") \\\n",
    "    .setMaster('spark://localhost:7077') \\\n",
    "    .set(\"spark.executor.memory\", \"2g\") \\\n",
    "    .set(\"spark.driver.memory\", \"2g\") \\\n",
    "    .set(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    "    .set(\"spark.memory.offHeap.size\",\"1g\") \\\n",
    "    .set(\"spark.sql.warehouse.dir\", \"hdfs://localhost:9000/user/hive/warehouse\")\n",
    "\n",
    "#.set(\"hive.metastore.uris\", \"thrift://127.0.0.1:9083\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm metastore_db -rf || rm derby.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "hiveContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hiveContext.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"overwrite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = hiveContext.read \\\n",
    "    .option(\"wholeFile\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\n",
    "        \"file:///home/juliotorres/hackaton/data/Candidates.csv\",\n",
    "        sep=',', quote='\"'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+------+----------+----------+----+---+----------+--------+-------------+--------------------+----+----+-----+-----+----+----+----+----+----+----+----+\n",
      "|_c0|                 _c1|            _c2|   _c3|       _c4|       _c5| _c6|_c7|       _c8|     _c9|         _c10|                _c11|_c12|_c13| _c14| _c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|\n",
      "+---+--------------------+---------------+------+----------+----------+----+---+----------+--------+-------------+--------------------+----+----+-----+-----+----+----+----+----+----+----+----+\n",
      "|  6|   6.email@gmail.com|     SantiLopez|     L|3755544583|1996-09-14|null|  0|4481457334|Colombia|     Medellin|                null|null|null|false|false|null|null|null|null|  []|  []| []\r",
      "|\n",
      "|  7|7.email@magnetosy...|          Simon| Hoyos|3122797544|1989-08-12|null|  0| 428632826|Colombia|     Medellin|                null|null|null|false|false|null|null|null|null|  []|  []| []\r",
      "|\n",
      "|  8|8.email@innventto...|      Sebastian|     T|3413519737|1991-11-10|null|  0|6673576163|Colombia|  Cravo Norte|Bachillerato comp...|null|null|false|false|null|null|null|null|  []|  []| []\r",
      "|\n",
      "|  1|   1.email@gmail.com|SebastianTorres|     T|3452305729|1991-11-10|null|  1|7892608945|Colombia|Puerto Narino|Bachillerato comp...|null|null|false|false|null|null|null|null|  []|  []| []\r",
      "|\n",
      "|  9|9.email@innventto...|         Felipe|Ocampo|3957730014|1994-10-25|null|  1|2050188588|Colombia|     Medellin|Bachillerato comp...|null|null|false|false|null|null|null|null|  []|  []| []\r",
      "|\n",
      "+---+--------------------+---------------+------+----------+----------+----+---+----------+--------+-------------+--------------------+----+----+-----+-----+----+----+----+----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidates.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o93.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 10, 192.168.43.148, executor 0): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/juliotorres/candidates/_temporary/0/_temporary/attempt_20200310151730_0004_m_000000_10/part-00000-86c0e3ec-118b-416b-b1aa-d23b3ff0db93-c000.snappy.parquet could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy17.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy18.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1455)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1251)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448)\n\tSuppressed: java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.error(ParquetFileWriter.java:168)\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.startBlock(ParquetFileWriter.java:160)\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter.startBlock(ParquetFileWriter.java:291)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:171)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\t\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:83)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$1.apply$mcV$sp(FileFormatWriter.scala:250)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1403)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\t\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 33 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/juliotorres/candidates/_temporary/0/_temporary/attempt_20200310151730_0004_m_000000_10/part-00000-86c0e3ec-118b-416b-b1aa-d23b3ff0db93-c000.snappy.parquet could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy17.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy18.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1455)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1251)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448)\n\tSuppressed: java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.error(ParquetFileWriter.java:168)\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.startBlock(ParquetFileWriter.java:160)\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter.startBlock(ParquetFileWriter.java:291)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:171)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\t\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:83)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$1.apply$mcV$sp(FileFormatWriter.scala:250)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1403)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\t\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7340bfffcd9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcandidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/user/juliotorres/candidates\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o93.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 10, 192.168.43.148, executor 0): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/juliotorres/candidates/_temporary/0/_temporary/attempt_20200310151730_0004_m_000000_10/part-00000-86c0e3ec-118b-416b-b1aa-d23b3ff0db93-c000.snappy.parquet could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy17.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy18.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1455)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1251)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448)\n\tSuppressed: java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.error(ParquetFileWriter.java:168)\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.startBlock(ParquetFileWriter.java:160)\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter.startBlock(ParquetFileWriter.java:291)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:171)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\t\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:83)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$1.apply$mcV$sp(FileFormatWriter.scala:250)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1403)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\t\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 33 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/juliotorres/candidates/_temporary/0/_temporary/attempt_20200310151730_0004_m_000000_10/part-00000-86c0e3ec-118b-416b-b1aa-d23b3ff0db93-c000.snappy.parquet could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1832)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2591)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:880)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:517)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:507)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1034)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:922)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2833)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy17.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy18.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1455)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1251)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448)\n\tSuppressed: java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.error(ParquetFileWriter.java:168)\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.startBlock(ParquetFileWriter.java:160)\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter.startBlock(ParquetFileWriter.java:291)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:171)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\t\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:83)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$1.apply$mcV$sp(FileFormatWriter.scala:250)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1403)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\t\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "candidates.write.parquet(\"/user/juliotorres/candidates\", mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = candidates.selectExpr(\n",
    "    \"cast(_c0 as int) id\", \n",
    "    \"_c1 email\",\n",
    "    \"_c2 first_name\",\n",
    "    \"_c3 last_name\",\n",
    "    \"_c4 phone\",\n",
    "    \"TO_DATE(CAST(UNIX_TIMESTAMP(_c5, 'yyyy-MM-dd') AS TIMESTAMP)) as birthdate\",\n",
    "    \"_c6 gender\",\n",
    "    \"cast(_c7 as int) identification_type\",\n",
    "    \"_c8 identification_number\",\n",
    "    \"_c9 country_birth\",\n",
    "    \"_c10 city\",\n",
    "    \"_c11 education_level\",\n",
    "    \"cast(_c12 as int) salary\",\n",
    "    \"_c13 profile_description\",\n",
    "    \"cast(cast(_c14 as boolean) as int) withow_experience\",\n",
    "    \"cast(cast(_c15 as boolean) as int) withow_studies\",\n",
    "    \"_c16 sectors\",\n",
    "    \"cast(cast(_c17 as boolean) as int) title_of_profetion\",\n",
    "    \"_c18 civil_status\",\n",
    "    \"_c19 presentation\",\n",
    "    \"_c20 educational_institution\",\n",
    "    \"_c21 experience\",\n",
    "    \"regexp_replace(_c22, '\\r', '') profession\"\n",
    ").filter(\"length(_c2) < 100 and length(_c3) < 100\")\n",
    "\n",
    "ocandidates = candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiveContext.registerDataFrameAsTable(candidates, \"candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_m = hiveContext.sql(\"\"\"\n",
    "select percentile_approx(salary, 0.5) salary\n",
    "  from candidates\n",
    " where salary is not null\n",
    " \"\"\").collect()[0].salary\n",
    "\n",
    "salary_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = candidates \\\n",
    "    .selectExpr(\n",
    "        \"id\",\n",
    "    \n",
    "        \"\"\"email      <> '' and email is not null as has_email\"\"\",\n",
    "        \"\"\"first_name <> '' and first_name is not null as has_first_name\"\"\",\n",
    "        \"\"\"last_name  <> '' and last_name is not null as has_last_name\"\"\",\n",
    "        \"\"\"phone      <> '' and phone is not null as has_phone\"\"\",\n",
    "\n",
    "        \"\"\"CAST(datediff(\n",
    "            current_date(), \n",
    "            TO_DATE(CAST(UNIX_TIMESTAMP(birthdate,'yyyy-MM-dd') AS TIMESTAMP))\n",
    "           )/365 as integer) as age\"\"\",\n",
    "\n",
    "        \"\"\"case when gender = 'male' then 'm'\n",
    "                when gender = 'female' then 'f' \n",
    "           else 'u' end as gender\"\"\",\n",
    "\n",
    "        \"\"\"case when identification_type is null then 100 \n",
    "           else identification_type \n",
    "           end identification_type\"\"\",\n",
    "\n",
    "        \"\"\"identification_number <> '' and \n",
    "           identification_number is not null \n",
    "           as has_identification_number\"\"\",\n",
    "\n",
    "        \"\"\"case when city <> '' and city is not null then city \n",
    "           else 'unknow' end as city\"\"\",\n",
    "\n",
    "        \"\"\"case when education_level <> '' and education_level is not null then education_level \n",
    "           else 'unknow' end as education_level\"\"\",\n",
    "\n",
    "        \"\"\"case when salary is null then %s \n",
    "           else salary end as salary\"\"\" % salary_m,\n",
    "\n",
    "        \"\"\"case when profile_description is null then ''\n",
    "           else profile_description end as profile_description\"\"\",\n",
    "\n",
    "        \"\"\"case when withow_experience is null then false \n",
    "           else withow_experience = 1 end withow_experience\"\"\",\n",
    "\n",
    "        \"\"\"case when withow_studies is null then false \n",
    "           else withow_studies = 1 end withow_studies\"\"\",\n",
    "\n",
    "        \"\"\"case when sectors is null then 'oficios varios'\n",
    "           else sectors end as sectors\"\"\",\n",
    "\n",
    "        \"\"\"case when title_of_profetion is null then false \n",
    "           else title_of_profetion = 1 end title_of_profetion\"\"\",\n",
    "\n",
    "        \"\"\"case when civil_status is null then 'unknow'\n",
    "           else civil_status end civil_status\"\"\",\n",
    "\n",
    "        \"\"\"presentation is not null has_presentation\"\"\",\n",
    "        \"presentation\",\n",
    "        # jalar metadata\n",
    "\n",
    "        \"educational_institution\", \"experience\", \"profession\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [ \n",
    "    'id', 'has_email', 'has_first_name', 'has_last_name', 'has_phone', 'age', 'gender', 'identification_type',\n",
    "    'has_identification_number', 'city', 'education_level', 'salary', 'profile_description',\n",
    "    'withow_experience', 'withow_studies', 'sectors', 'title_of_profetion', 'civil_status', 'presentation'\n",
    "]\n",
    "#candidates.columns[1:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_study(r):\n",
    "    educational_institution = []\n",
    "    try:\n",
    "        if r.educational_institution is not None:\n",
    "            educational_institution = json.loads(r.educational_institution)\n",
    "    except:\n",
    "        pass\n",
    "    return {\n",
    "        'institute_complete': [ \n",
    "            x['institute'] or ''\n",
    "            for x in educational_institution\n",
    "            if x['in_progress'] == False\n",
    "        ],\n",
    "        'institute_incomplete': [ \n",
    "            x['institute'] or ''\n",
    "            for x in educational_institution\n",
    "            if x['in_progress'] == True\n",
    "        ],\n",
    "        'title_complete': [\n",
    "            x['title'] or ''\n",
    "            for x in educational_institution\n",
    "            if x['in_progress'] == False\n",
    "        ],\n",
    "        'title_incomplete': [\n",
    "            x['title'] or ''\n",
    "            for x in educational_institution\n",
    "            if x['in_progress'] == True\n",
    "        ],\n",
    "        'study_type_complete': [ \n",
    "            x['study_type'] or ''\n",
    "            for x in educational_institution\n",
    "            if x['in_progress'] == False\n",
    "        ],\n",
    "        'study_type_incomplete': [ \n",
    "            x['study_type'] or ''\n",
    "            for x in educational_institution\n",
    "            if x['in_progress'] == True\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = re.compile(\"\\s+\")\n",
    "\n",
    "def parse_experience(r):\n",
    "    experience = []\n",
    "    try:\n",
    "        if r.experience is not None:\n",
    "            experience = json.loads(r.experience)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    company = []\n",
    "    position = []\n",
    "    description = []\n",
    "    \n",
    "    total_experience = 0\n",
    "        \n",
    "    if len(experience) > 0: \n",
    "        for x in experience:\n",
    "            start_date = datetime.strptime(x['start_date'], '%Y-%m-%d')\n",
    "            \n",
    "            if x['at_present'] != True:\n",
    "                end_date = datetime.strptime(x['end_date'], '%Y-%m-%d')\n",
    "            else:\n",
    "                end_date = datetime.now()\n",
    "                \n",
    "            if start_date is not None and end_date is not None:\n",
    "                num_months = (end_date.year - start_date.year) * 12 + (end_date.month - start_date.month)\n",
    "            else:\n",
    "                num_months = 1\n",
    "            \n",
    "            if x['company']:\n",
    "                company.append((x['company'] + ' ') * num_months)\n",
    "            if x['position']:\n",
    "                position.append((x['position'] + ' ') * num_months)\n",
    "            if x['description']:\n",
    "                description.append((replace.sub(' ', x['description']) + ' ') * num_months)\n",
    "                \n",
    "            total_experience+= num_months\n",
    "                \n",
    "            \n",
    "    return {\n",
    "        'company':     company,\n",
    "        'position':    position,\n",
    "        'description': description,\n",
    "        'experience':  total_experience or 0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_experience(candidates.where(\"experience <> '[]'\").limit(1).collect()[0])['experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_profession(r):\n",
    "    profession = []\n",
    "    try:\n",
    "        if r.profession is not None:\n",
    "            profession = json.loads(r.profession)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(profession) == 0:\n",
    "        evaluation = [0]\n",
    "    elif len(profession) < 3:\n",
    "        evaluation = [\n",
    "            sum([\n",
    "                x['percent'] or 0\n",
    "                for x in element['psy_tests'] \n",
    "            ])\n",
    "            for element in profession\n",
    "        ]\n",
    "    else:\n",
    "        evaluation = [\n",
    "            sum([\n",
    "                x['percent'] or 0\n",
    "                for x in element['psy_tests'] \n",
    "                if x['percent'] is not None\n",
    "            ])\n",
    "            for element in profession[-3:]\n",
    "        ]\n",
    "    \n",
    "    evaluation = statistics.median(evaluation) or 0\n",
    "    \n",
    "    return {\n",
    "        'city_name':     [ x['city_name'] or '' for x in profession ],\n",
    "        'position_name': [ x['position_name'] or '' for x in profession ],\n",
    "        'evaluation':    int(evaluation)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediana = candidates \\\n",
    "    .where('age >= 18 or age > 100') \\\n",
    "    .selectExpr(\"percentile_approx(age, .5) mediana\") \\\n",
    "    .collect()[0].mediana\n",
    "mediana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(r):\n",
    "    s = parse_study(r)\n",
    "    e = parse_experience(r)\n",
    "    p = parse_profession(r)\n",
    "    age = r.age or mediana\n",
    "    return (\n",
    "        r.id,r.has_email, r.has_first_name, r.has_last_name, r.has_phone, \n",
    "        (age if age <= 80 else mediana) if age >= 18 else mediana,\n",
    "        r.gender, r.identification_type, r.has_identification_number, \n",
    "        r.city, r.education_level, r.salary, r.profile_description, \n",
    "        r.withow_experience, r.withow_studies, r.sectors, \n",
    "        r.title_of_profetion, r.civil_status, r.presentation,\n",
    "        ' '.join(s['institute_complete']), \n",
    "        ' '.join(s['institute_incomplete']),\n",
    "        ' '.join(s['title_complete']),\n",
    "        ' '.join(s['title_incomplete']),\n",
    "        ' '.join(s['study_type_complete']),\n",
    "        ' '.join(s['study_type_incomplete']),\n",
    "        ' '.join(e['company']),\n",
    "        ' '.join(e['position']),\n",
    "        ' '.join(e['description']),\n",
    "        #' '.join(e['experience']),\n",
    "        ' '.join(p['city_name']),\n",
    "        ' '.join(p['position_name']),\n",
    "        p['evaluation'],\n",
    "    )\n",
    "\n",
    "candidates = candidates \\\n",
    "    .rdd \\\n",
    "    .map(parse) \\\n",
    "    .toDF(fields+[\n",
    "        'educational_institute_complete', \n",
    "        'educational_institute_incomplete',\n",
    "        'educational_title_complete', \n",
    "        'educational_title_incomplete',\n",
    "        'educational_study_type_complete',\n",
    "        'educational_study_type_incomplete',\n",
    "\n",
    "        'experience_company', \n",
    "        'experience_position', \n",
    "        'experience_description',\n",
    "        #'experience_total',\n",
    "\n",
    "        'profesion_city_name',\n",
    "        'profesion_position_name',\n",
    "        'profesion_evaluation'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.write.parquet(\"/user/juliotorres/candidates\", mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiveContext.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "candidates.write.jdbc(url=url, table=\"candidates\", mode=mode, properties=properties)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando diccionario de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counts(r):\n",
    "    return r\n",
    "\n",
    "word_counts(candidates.where('experience_description').limit(5).collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos vacants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacants = hiveContext.read \\\n",
    "    .option(\"wholeFile\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"file:///home/juliotorres/hackaton/data/Vacants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relacion = vacants \\\n",
    "    .where('_c5 is not null') \\\n",
    "    .selectExpr(\"cast(_c5 as int) _c5\", \"cast(_c4 as int) _c4\") \\\n",
    "    .selectExpr(\"_c5 / _c4 relacion\" ) \\\n",
    "    .selectExpr(\"percentile_approx(relacion, .7) relacion\") \\\n",
    "    .collect()[0].relacion\n",
    "\n",
    "relacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacants = vacants.selectExpr(\"*\", \"regexp_replace(_c18, '\\r', '') _c19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ x['_c15'] for x in vacants.select(\"_c15\").distinct().collect()[:5] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacants = vacants.selectExpr(\n",
    "    \"cast(_c0 as int) id\", \n",
    "    \"case when _c1 is not null then _c1 else '' end title\",\n",
    "    \"case when _c2 is not null then _c2 else '' end description\",\n",
    "    \"case when _c3 is not null then _c3 else '' end salary_type\",\n",
    "    \"case when _c4 is not null then cast(_c4 as int) else '' end min_salary\",\n",
    "    \"\"\"case when _c5 is not null then cast(_c5 as int) \n",
    "       else cast(cast(_c4 as int) * %s as int) end as max_salary\"\"\" % (relacion,),\n",
    "    \"cast(_c6 as int) status\",\n",
    "    \"TO_DATE(CAST(UNIX_TIMESTAMP(_c7, 'yyyy-MM-dd') AS TIMESTAMP)) as created_at\",\n",
    "    \"_c8 company\",\n",
    "    \"_c9 education_level\",\n",
    "    \"cast(cast(_c10 as boolean) as int) agree\",\n",
    "    \"case when _c11 is not null then _c11 else '' end requirements\",\n",
    "    \"TO_DATE(CAST(UNIX_TIMESTAMP(_c12, 'yyyy-MM-dd') AS TIMESTAMP)) publish_state\",\n",
    "    \"cast(cast(_c13 as boolean) as int) confidential\",\n",
    "    \"TO_DATE(CAST(UNIX_TIMESTAMP(_c14, 'yyyy-MM-dd') AS TIMESTAMP)) expiration_date\",\n",
    "    \"case when _c15 is not null then _c15 else '' end experience_and_positions\",\n",
    "    \"case when _c16 is not null then _c16 else '' end knowledge_and_skills\",\n",
    "    \"case when _c17 is not null then _c17 else '' end titles_and_studies\",\n",
    "    \"case when _c19 <> '' then cast(_c19 as int) else 0 end number_of_quotas\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacants.toPandas().experience_and_positions.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = vacants.columns\n",
    "fields[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_description(t):\n",
    "    return ' '.join(re.split('\\s+', (html2text.html2text(t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(r):\n",
    "    return (\n",
    "        r.id,r.title,parse_description(r.description),r.salary_type,r.min_salary,r.max_salary,\n",
    "        r.status,r.created_at,r.company,r.education_level,r.agree, r.requirements,\n",
    "        r.publish_state,r.confidential,r.expiration_date or '',\n",
    "        r.experience_and_positions,r.knowledge_and_skills,r.titles_and_studies,\n",
    "        r.number_of_quotas,\n",
    "    )\n",
    "\n",
    "vacants = vacants.rdd.map(parse).toDF(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacants.write.parquet(\"/user/juliotorres/vacants\", mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "vacants.write.jdbc(url=url, table=\"vacants\", mode=mode, properties=properties)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = hiveContext.read \\\n",
    "    .option(\"wholeFile\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"file:///home/juliotorres/hackaton/data/Stages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = stages.selectExpr(\n",
    "    \"cast(_c0 as int) id\", \n",
    "    \"case when _c1 is not null then _c1 else '' end title\",\n",
    "    \"cast(cast(_c2 as boolean) as int) send_sms\",\n",
    "    \"cast(cast(_c3 as boolean) as int) send_email\",\n",
    "    \"cast(cast(_c4 as boolean) as int) send_call\",\n",
    "    \"cast(_c5 as int) created_at\",\n",
    "    \"cast(_c6 as int) status\",\n",
    "    \"cast(0+_c7 as int) discard_type\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages.write.parquet(\"/user/juliotorres/stages\", mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "stages.write.jdbc(url=url, table=\"stages\", mode=mode, properties=properties)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications = hiveContext.read \\\n",
    "    .option(\"wholeFile\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"file:///home/juliotorres/hackaton/data/Applications.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications = applications.selectExpr(\n",
    "    \"cast(_c0 as int) id\", \n",
    "    \"cast(_c1 as int) as vacant_id\",\n",
    "    \"cast(_c2 as int) candidate_id\",\n",
    "    \"TO_DATE(CAST(UNIX_TIMESTAMP(_c3, 'yyyy-MM-dd') AS TIMESTAMP)) as created_at\",\n",
    "    \"_c4 status\",\n",
    "    \"regexp_replace(_c5, '\\r', '') register_type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications.write.parquet(\"/user/juliotorres/applications\", mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "applications.write.jdbc(url=url, table=\"applications\", mode=mode, properties=properties)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos applicationStages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicationStages = hiveContext.read \\\n",
    "    .option(\"wholeFile\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"file:///home/juliotorres/hackaton/data/ApplicationStages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicationStages = applicationStages.selectExpr(\n",
    "    \"cast(_c0 as int) id\", \n",
    "    \"cast(_c1 as int) stage_id\",\n",
    "    \"cast(_c2 as int) application_id\",\n",
    "    \"TO_DATE(CAST(UNIX_TIMESTAMP(_c3, 'yyyy-MM-dd') AS TIMESTAMP)) created_at\",\n",
    "    \"regexp_replace(_c4, '\\r', '') status\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicationStages.write.parquet(\"/user/juliotorres/applicationStages\", mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "applicationStages.write.jdbc(url=url, table=\"applicationStages\", mode=mode, properties=properties)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiveContext.registerDataFrameAsTable(stages, 'stages')\n",
    "hiveContext.registerDataFrameAsTable(vacants, 'vacants')\n",
    "hiveContext.registerDataFrameAsTable(candidates, 'candidates')\n",
    "hiveContext.registerDataFrameAsTable(applications, 'applications')\n",
    "hiveContext.registerDataFrameAsTable(applicationStages, 'applicationStages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = hiveContext.sql(\"\"\"\n",
    "    select * \n",
    "      from candidates b\n",
    " left join applications a on a.candidate_id = b.id\n",
    " left join vacants c on a.vacant_id = c.id\n",
    " left join applicationStages d on d.application_id = a.id\n",
    " left join stages e on d.stage_id = e.id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_c = candidates.columns\n",
    "candidates_c[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications_c = [ 'application_' + x for x in applications.columns ]\n",
    "applications_c[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacants_c = [ 'vacant_' + x for x in vacants.columns ]\n",
    "vacants_c[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicationStages_c = [ 'applicationStage_' + x for x in applicationStages.columns ]\n",
    "applicationStages_c[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_c = [ 'stage_' + x for x in stages.columns ]\n",
    "stages_c[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = candidates_c + applications_c + vacants_c + applicationStages_c + stages_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history.toDF(*fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.write.parquet(\"/user/juliotorres/history\", mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiveContext.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
